üß† System Name: ATLAS (Agentic Task Logic & Analysis System)
GitHub: https://github.com/CuriosityQuantified/ATLAS
Email: CuriosityQuantified@gmail.com
Username: CuriosityQuantified
‚∏ª

üéØ Primary Objective

Enable modular reasoning and structured content generation by decomposing complex tasks (e.g., research synthesis, decision-making, strategy docs) into specialized sub-processes governed by autonomous but coordinated agent teams.

‚∏ª
üèóÔ∏è Core System Architecture

üîπ Top-Level Coordinator: Global Supervisor Agent
	‚Ä¢	Owns the full task.
	‚Ä¢	Breaks task into stages: Research ‚Üí Analysis ‚Üí Writing ‚Üí Rating.
	‚Ä¢	Assigns and routes responsibilities to team-level Supervisor Agents.
	‚Ä¢	Maintains system memory and task context.
	‚Ä¢	Can issue revisions based on Rating Agent feedback.

‚∏ª

üß© Sub-Teams (each is a fully modular unit)

1. üïµÔ∏è‚Äç‚ôÇÔ∏è Research Team
	‚Ä¢	Team Supervisor Agent (Research Supervisor)
	‚Ä¢	Breaks down research goals into sub-queries.
	‚Ä¢	Assigns subtasks to Research Worker Agents.
	‚Ä¢	Aggregates and filters findings into structured output.
	‚Ä¢	Ensures relevance, reliability, and diversity of sources.
	‚Ä¢	Research Worker Agents
	‚Ä¢	Query tools (web, vector DB, document loaders, APIs).
	‚Ä¢	Summarize raw content.
	‚Ä¢	Extract facts, quotes, references.

‚∏ª

2. üìä Analysis Team
	‚Ä¢	Team Supervisor Agent (Analysis Supervisor)
	‚Ä¢	Interprets research outputs.
	‚Ä¢	Creates analytical frameworks (e.g., pros/cons, SWOT, cause-effect).
	‚Ä¢	Coordinates worker-level analysis tasks.
	‚Ä¢	Analysis Worker Agents
	‚Ä¢	Perform logical reasoning, sentiment analysis, comparative judgment.
	‚Ä¢	Run specific analytical methods (e.g., scorecard evaluation, modeling).
	‚Ä¢	Challenge or validate claims and identify contradictions.

‚∏ª

3. ‚úçÔ∏è Writing Team
	‚Ä¢	Team Supervisor Agent (Writing Supervisor)
	‚Ä¢	Converts structured findings into written content.
	‚Ä¢	Coordinates sections (e.g., intro, body, conclusion).
	‚Ä¢	Manages coherence, tone, length, formatting.
	‚Ä¢	Writing Worker Agents
	‚Ä¢	Draft paragraphs or sections.
	‚Ä¢	Rewrite for clarity, tone, or audience.
	‚Ä¢	Apply templates or writing styles (formal, casual, persuasive).

‚∏ª

4. üìà Rating/Feedback Team
	‚Ä¢	Team Supervisor Agent (Rating Supervisor)
	‚Ä¢	Breaks output into evaluation criteria (e.g., logic, style, accuracy).
	‚Ä¢	Assigns criteria to Rating Worker Agents.
	‚Ä¢	Aggregates ratings and generates revision instructions.
	‚Ä¢	Rating Worker Agents
	‚Ä¢	Score outputs quantitatively (e.g., 1‚Äì10) and qualitatively.
	‚Ä¢	Suggest revisions.
	‚Ä¢	Detect hallucination, factual issues, and stylistic inconsistency.

‚∏ª

üîÅ Feedback Loop
	‚Ä¢	Rating Team output flows back to the Global Supervisor Agent.
	‚Ä¢	Supervisor may:
	‚Ä¢	Reassign tasks to fix issues.
	‚Ä¢	Request rewrites from Writing Team.
	‚Ä¢	Trigger new research or analysis to address gaps.
	‚Ä¢	Log outputs for memory and future retrieval.

‚∏ª

üß† Memory & Context Handling
	‚Ä¢	Persistent Memory Store (e.g., vector DB + structured JSON object)
	‚Ä¢	Stores findings, decisions, rationale, interim drafts, ratings.
	‚Ä¢	Each Agent/Team maintains a local scratchpad to avoid flooding global context window.
	‚Ä¢	Uses LangGraph-style state transition control (optional implementation idea).

‚∏ª

üìÇ Output Formats
	‚Ä¢	Structured JSON Object containing:
	‚Ä¢	final_output: the clean written artifact.
	‚Ä¢	evidence_chain: traceable research steps.
	‚Ä¢	analysis_summary: decision logic, pros/cons, etc.
	‚Ä¢	ratings: score and revision logs.
	‚Ä¢	Optionally also export:
	‚Ä¢	Mind map (Freeplane-compatible).
	‚Ä¢	Neo4j knowledge graph.
	‚Ä¢	Audio/podcast (via Mozilla TTS + Podcast Generator).

‚∏ª

üì¶ Modularity and Composability
	‚Ä¢	Each sub-team can be reused for other projects.
	‚Ä¢	Plug-and-play structure: swap out writing agents, analytical methods, or rating criteria.
	‚Ä¢	Could be instantiated recursively (i.e., a Research Team can use its own analysis/writing loop for detailed outputs).

‚∏ª

‚ö†Ô∏è Key Design Considerations
	‚Ä¢	Agent context windows: Use chunking, memory optimization, and scratchpad logic.
	‚Ä¢	Latency vs. quality: Configure parallelism and depth (e.g., 3 research agents at once).
	‚Ä¢	Grounding: Inject verified sources to reduce hallucinations.
	‚Ä¢	Autonomy boundaries: Supervisors must know when to escalate to the Global Supervisor.

‚∏ª

üìà Ideal Use Cases
	‚Ä¢	Strategic briefings
	‚Ä¢	Product requirement documents
	‚Ä¢	Investment memos
	‚Ä¢	Market analysis reports
	‚Ä¢	Decision justifications with source trail

MCP servers for tools
https://mcp.deepwiki.com/
https://github.com/browserbase/mcp-server-browserbase
https://github.com/digma-ai/digma-mcp-server
https://github.com/GongRzhe/Office-PowerPoint-MCP-Server
https://github.com/github/github-mcp-server
https://github.com/neo4j-contrib/mcp-neo4j/tree/main/servers/mcp-neo4j-cloud-aura-api
https://docs.firecrawl.dev/mcp
https://github.com/haris-musa/excel-mcp-server
https://github.com/anaisbetts/mcp-youtube
https://github.com/JetBrains/mcp-jetbrains
https://github.com/makenotion/notion-mcp-server
https://github.com/ppl-ai/modelcontextprotocol
https://github.com/microsoft/playwright-mcp
https://github.com/e2b-dev/mcp-server
https://github.com/supabase-community/supabase-mcp
https://github.com/modelcontextprotocol/servers/tree/main/src/filesystem

For agent knowledge base / knowledge graph / hybrid rag
/Users/nicholaspate/Documents/raw-mas/Overnight Strategist - Strategist Toolkit V3 copy.pdf